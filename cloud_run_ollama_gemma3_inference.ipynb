{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangalo20/latitude/blob/main/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Serving Gemma 3 on Cloud Run\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/jmwai/gemma3-cloud-run-demo/blob/main/cloud_run_ollama_gemma3_inference.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83b98b0ba19c"
      },
      "source": [
        "<img src=\"https://ollama.com/public/ollama.png\" height=\"200px\" alignment=\"center\"/>\n",
        "<img src=\"https://cloud.google.com/static/architecture/images/ac-page-icons/card_google_cloud_partner.svg\" height=\"200px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Vlad Kolesnikov](https://github.com/vladkol) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccd500ae19b5"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "  [**Gemma 3**](https://ai.google.dev/gemma) is a new generation of open models developed by Google. It is a collection of lightweight, state-of-the-art open models built from the same research and technology that powers our Gemini 2.0 models. Gemma 3 comes in a range of sizes (270M, 1B, 4B, 12B and 27B), allowing you to choose the best model for your specific hardware and performance needs. Gemma 3 models are available through platforms like Google AI Studio, Vertex AI, Kaggle, and Hugging Face.\n",
        "\n",
        "> **[Cloud Run](https://cloud.google.com/run)**:\n",
        "It's a serverless platform by Google Cloud for running containerized applications. It automatically scales and manages infrastructure, supporting various programming languages. Cloud Run now offers GPU acceleration for AI/ML workloads. With 30 seconds to the first token, Cloud Run is a perfect platform for serving lightweight models like Gemma.\n",
        "\n",
        "> **Note:** GPU support in Cloud Run is in preview. To use the GPU feature, you must request `Total Nvidia L4 GPU allocation, per project per region` quota under Cloud Run in the [Quotas and system limits page](https://cloud.google.com/run/quotas#increase).\n",
        "\n",
        "\n",
        "> **[Ollama](ollama.com)**: is an open-source tool for easily running and deploying large language models locally. It offers simple management and usage of LLMs on personal computers or servers.\n",
        "\n",
        "This notebook showcases how to deploy [Google Gemma 3](https://developers.googleblog.com/en/introducing-gemma3) in Cloud Run, with the objective to build a simple API for chat or RAG applications.\n",
        "\n",
        "By the end of this notebook, you will learn how to:\n",
        "\n",
        "1. Deploy Google Gemma 3 as an OpenAI-compatible API on Cloud Run using Ollama.\n",
        "2. Build a custom container with Ollama to deploy any Large Language Model (LLM) of your choice.\n",
        "3. Make requests to an API hosted on Cloud Run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOiPjM5DEPhK"
      },
      "source": [
        "### Install Google Cloud SDK\n",
        "\n",
        "Make sure you Google Cloud SDK is installed (try running `gcloud version`) or [install it](https://cloud.google.com/sdk/docs/install) before executing this notebook.\n",
        "\n",
        "> If you are running in Colab or Vertex AI workbench, you have Google Cloud SDK installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfAVa08RDDJB"
      },
      "source": [
        "### Choose a model, a project, and a region to host the model\n",
        "\n",
        "Choose a Gemma 3 model to use, a Google Cloud project to host your Cloud Run service, and a region to host it in.\n",
        "For this demo we will chose the gemma3:270m model. If you cannot attach a GPU to your Cloud Run instance, chose the gemma3:270m and remove the GPU requirements in the cloud run command\n",
        "\n",
        "If you don't have a project yet:\n",
        "\n",
        "1. [Create a project](https://console.cloud.google.com/projectcreate) in the Google Cloud Console.\n",
        "2. Copy your `Project ID` from the project's [Settings page](https://console.cloud.google.com/iam-admin/settings).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "id": "TV0pbqJHDDJB"
      },
      "outputs": [],
      "source": [
        "# { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "\n",
        "PROJECT_ID = \"sangalo-personal\"  # @param {type:\"string\", isTemplate: true}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\", isTemplate: true}\n",
        "MODEL = \"gemma3:270m\" # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "if PROJECT_ID == \"[your-project-id]\" or not PROJECT_ID:\n",
        "    print(\"Please specify your project id in PROJECT_ID variable.\")\n",
        "    raise KeyboardInterrupt\n",
        "\n",
        "MODEL_NAME_ESCAPED = MODEL.translate(str.maketrans(\".:/\", \"---\"))\n",
        "SERVICE_NAME = f\"ollama--{MODEL_NAME_ESCAPED}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xc8Jm1P3Y7fs"
      },
      "outputs": [],
      "source": [
        "!gcloud auth print-identity-token -q &> /dev/null || gcloud auth login --project=\"{PROJECT_ID}\" --update-adc --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l728UOEPDDJB"
      },
      "source": [
        "## Prepare container image\n",
        "\n",
        "First, let's create a Docker file for a container with the model embedded into it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "glBn9gPKDDJB",
        "outputId": "bc0f390a-25d9-4758-fe26-3e2e5a06e967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM ollama/ollama:latest\n",
        "\n",
        "ARG MODEL\n",
        "\n",
        "# Set the model name\n",
        "ENV MODEL=$MODEL\n",
        "\n",
        "# Set the host and port to listen on\n",
        "ENV OLLAMA_HOST 0.0.0.0:8080\n",
        "\n",
        "# Set the directory to store model weight files\n",
        "ENV OLLAMA_MODELS /models\n",
        "\n",
        "# Reduce the verbosity of the logs\n",
        "ENV OLLAMA_DEBUG false\n",
        "\n",
        "# Do not unload model weights from the GPU\n",
        "ENV OLLAMA_KEEP_ALIVE -1\n",
        "\n",
        "# Start the ollama server and download the model weights\n",
        "RUN ollama serve & sleep 5 && ollama pull $MODEL\n",
        "\n",
        "# At startup time we start the server and run a dummy request\n",
        "# to request the model to be loaded in the GPU memory\n",
        "ENTRYPOINT [\"/bin/sh\"]\n",
        "CMD [\"-c\", \"ollama serve  & (ollama run $MODEL 'Say one word' &) && wait\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4gS8ovMDDJB"
      },
      "source": [
        "Second, we create a Cloud Build file to use for building and pushing our container image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1dV1_cMDDDJB",
        "outputId": "4b3f65cb-8d19-41d9-ea93-38d943e5c3f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cloudbuild.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile cloudbuild.yaml\n",
        "\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  id: build\n",
        "  entrypoint: 'bash'\n",
        "  args:\n",
        "    - -c\n",
        "    - |\n",
        "        docker buildx build --tag=${_IMAGE} --build-arg MODEL=${_MODEL} .\n",
        "\n",
        "images: [\"${_IMAGE}\"]\n",
        "\n",
        "substitutions:\n",
        "  _IMAGE: '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_AR_REPO_NAME}/${_SERVICE_NAME}'\n",
        "\n",
        "options:\n",
        "  dynamicSubstitutions: true\n",
        "  machineType: \"E2_HIGHCPU_32\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbDiABJcDDJC"
      },
      "source": [
        "## Build Container Image and Deploy Cloud Run Service\n",
        "\n",
        "We are ready to build our container image and deploy Cloud Run service.\n",
        "\n",
        "The script below performs the following actions:\n",
        "\n",
        "* Enables necessary APIs.\n",
        "* Creates an Artifact Repository for the image.\n",
        "* Creates a Service Account for the service.\n",
        "* Submits a Cloud Build job to create and push the container image.\n",
        "* Deploys the Cloud Run service.\n",
        "\n",
        "> The script may take 10-45 minutes to finish.\n",
        "\n",
        "Note the following important flags in Cloud Build deployment command:\n",
        "\n",
        "* `--concurrency 4` is set to match the value of the environment variable `OLLAMA_NUM_PARALLEL`.\n",
        "* `--gpu 1` with `--gpu-type nvidia-l4` assigns 1 NVIDIA L4 GPU to every Cloud Run instance in the service.\n",
        "`--no-allow-authenticated` restricts unauthenticated access to the service.\n",
        "By keeping the service private, you can rely on Cloud Run's built-in [Identity and Access Management (IAM)](https://cloud.google.com/iam) authentication for service-to-service communication.\n",
        "* `--no-cpu-throttling` is required for enabling GPU.\n",
        "* `--service-account` the service identity of the service.\n",
        "* `--max-instances` sets maximum number of instances of the service.\n",
        "It has to be equal to or lower than your project's NVIDIA L4 GPU (`Total Nvidia L4 GPU allocation, per project per region`) quota.\n",
        "\n",
        "For optimal GPU utilization, increase `--concurrency`, keeping it within twice the value of `OLLAMA_NUM_PARALLEL`.\n",
        "While this leads to request queuing in Ollama, it can help improve utilization:\n",
        "Ollama instances can immediately process requests from their queue, and the queues help absorb traffic spikes.\n",
        "\n",
        "#### If your cloud credits don't allow you to attache a GPU, change to Gemma 270m variant and deploy without GPU requirement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TXg7IYU1DDJC",
        "outputId": "6e179184-c075-4686-f9b6-a2578cbf3747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deploy.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile deploy.sh\n",
        "\n",
        "PROJECT_ID=$1\n",
        "REGION=$2\n",
        "MODEL_ID=\"${3}\"\n",
        "SERVICE_NAME=\"${4}\"\n",
        "AR_REPO_NAME=\"ollama-repo\"\n",
        "SERVICE_ACCOUNT=\"ollama-cloud-run-sa\"\n",
        "SERVICE_ACCOUNT_ADDRESS=\"${SERVICE_ACCOUNT}@$PROJECT_ID.iam.gserviceaccount.com\"\n",
        "MAX_INSTANCES=1 # Adjust this value to match your Cloud Run L4 GPU quota (\"Total Nvidia L4 GPU allocation, per project per region\", NvidiaL4GpuAllocPerProjectRegion, run.googleapis.com/nvidia_l4_gpu_allocation)\n",
        "\n",
        "echo \"Enabling APIs in project ${PROJECT_ID}.\"\n",
        "gcloud services enable run.googleapis.com \\\n",
        "    cloudbuild.googleapis.com \\\n",
        "    artifactregistry.googleapis.com \\\n",
        "    --project ${PROJECT_ID} \\\n",
        "    --quiet\n",
        "\n",
        "set -e\n",
        "\n",
        "# Creating the service account if doesn't exist.\n",
        "sa_list=$(gcloud iam service-accounts list --quiet --format 'value(email)' --project $PROJECT_ID --filter=email:$SERVICE_ACCOUNT@$PROJECT_ID.iam.gserviceaccount.com 2>/dev/null)\n",
        "if [ -z \"${sa_list}\" ]; then\n",
        "    echo \"Creating Service Account ${SERVICE_ACCOUNT}.\"\n",
        "    gcloud iam service-accounts create $SERVICE_ACCOUNT \\\n",
        "        --project ${PROJECT_ID} \\\n",
        "        --display-name=\"${SERVICE_ACCOUNT} - Cloud Run Service Account\"\n",
        "fi\n",
        "\n",
        "# Creating the Artifacts Repository if doesn't exist\n",
        "repo_list=$(gcloud artifacts repositories list --format 'value(name)' --filter=name=\"projects/${PROJECT_ID}/locations/${REGION}/repositories/${AR_REPO_NAME}\" --project ${PROJECT_ID} --quiet --location ${REGION} 2>/dev/null)\n",
        "if [ -z \"${repo_list}\" ]; then\n",
        "    echo \"Creating Artifact Registry ${AR_REPO_NAME}.\"\n",
        "    gcloud artifacts repositories create $AR_REPO_NAME \\\n",
        "    --repository-format docker \\\n",
        "    --location ${REGION} \\\n",
        "    --project=${PROJECT_ID}\n",
        "fi\n",
        "\n",
        "echo \"Building container image.\"\n",
        "gcloud builds submit --config=cloudbuild.yaml --project=${PROJECT_ID} . \\\n",
        "    --suppress-logs \\\n",
        "    --substitutions \\\n",
        "  _AR_REPO_NAME=$AR_REPO_NAME,_REGION=$REGION,_SERVICE_NAME=$SERVICE_NAME,_MODEL=$MODEL_ID\n",
        "rm -f cloudbuild.yaml\n",
        "rm -f Dockerfile\n",
        "\n",
        "echo \"Deploying Service ${SERVICE_NAME}.\"\n",
        "gcloud beta run deploy $SERVICE_NAME \\\n",
        "    --project=${PROJECT_ID} \\\n",
        "    --image=${REGION}-docker.pkg.dev/$PROJECT_ID/$AR_REPO_NAME/$SERVICE_NAME \\\n",
        "    --service-account $SERVICE_ACCOUNT_ADDRESS \\\n",
        "    --cpu=8 \\\n",
        "    --memory=32Gi \\\n",
        "    --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n",
        "    --region ${REGION} \\\n",
        "    --no-allow-unauthenticated \\\n",
        "    --max-instances ${MAX_INSTANCES} \\\n",
        "    --no-cpu-throttling \\\n",
        "    --timeout 1h\n",
        "\n",
        "SERVICE_URL=$(gcloud run services describe ${SERVICE_NAME} --project=${PROJECT_ID} --region $REGION --format 'value(status.url)' --quiet)\n",
        "echo \"âœ… Success!\"\n",
        "echo \"ðŸš€ Service URL: ${SERVICE_URL}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e6L2dVGyOAxB",
        "outputId": "c7b8a94b-f0de-40f2-8526-8005aa05978d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling APIs in project sangalo-personal.\n",
            "Operation \"operations/acat.p2-389817060103-5cc5fba3-ebc2-4f49-80f5-7518cc7b8dd8\" finished successfully.\n",
            "Creating Service Account ollama-cloud-run-sa.\n",
            "Created service account [ollama-cloud-run-sa].\n",
            "Creating Artifact Registry ollama-repo.\n",
            "Create request issued for: [ollama-repo]\n",
            "Created repository [ollama-repo].\n",
            "Building container image.\n",
            "Creating temporary archive of 39 file(s) totalling 54.3 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://sangalo-personal_cloudbuild/source/1762592051.047713-26ab1a9e229a48759aebeb23082cde77.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/sangalo-personal/locations/global/builds/89fb566b-dc0d-43a1-8016-73edd251646e].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/89fb566b-dc0d-43a1-8016-73edd251646e?project=389817060103 ].\n",
            "Waiting for build to complete. Polling interval: 1 second(s).\n",
            "\n",
            "INFO: The service account running this build projects/sangalo-personal/serviceAccounts/389817060103-compute@developer.gserviceaccount.com does not have permission to write logs to Cloud Logging. To fix this, grant the Logs Writer (roles/logging.logWriter) role to the service account.\n",
            "\n",
            "1 message(s) issued.\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                          IMAGES                                                                                 STATUS\n",
            "89fb566b-dc0d-43a1-8016-73edd251646e  2025-11-08T08:54:21+00:00  4M24S     gs://sangalo-personal_cloudbuild/source/1762592051.047713-26ab1a9e229a48759aebeb23082cde77.tgz  us-central1-docker.pkg.dev/sangalo-personal/ollama-repo/ollama--gemma3-270m (+1 more)  SUCCESS\n",
            "Deploying Service ollama--gemma3-270m.\n",
            "Deploying container to Cloud Run service [\u001b[1mollama--gemma3-270m\u001b[m] in project [\u001b[1msangalo-personal\u001b[m] region [\u001b[1mus-central1\u001b[m]\n",
            "Service [\u001b[1mollama--gemma3-270m\u001b[m] revision [\u001b[1mollama--gemma3-270m-00001-cbq\u001b[m] has been deployed and is serving \u001b[1m100\u001b[m percent of traffic.\n",
            "Service URL: \u001b[1mhttps://ollama--gemma3-270m-389817060103.us-central1.run.app\u001b[m\n",
            "âœ… Success!\n",
            "ðŸš€ Service URL: https://ollama--gemma3-270m-whhs723r5a-uc.a.run.app\n"
          ]
        }
      ],
      "source": [
        "!/bin/bash ./deploy.sh \"{PROJECT_ID}\" \"{REGION}\" \"{MODEL}\" \"{SERVICE_NAME}\" && rm -f ./deploy.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgaJ62rmDDJC"
      },
      "source": [
        "\n",
        "## Test the deployed service\n",
        "\n",
        "Now, let's test the service you deployed.\n",
        "\n",
        "First, simply by using `cURL`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iX7LmWwGDDJC",
        "outputId": "7ecae857-2bc7-44da-f575-9ba7d31aa79f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"model\":\"gemma3:270m\",\"created_at\":\"2025-11-08T09:27:29.588728279Z\",\"response\":\"The sky is blue due to a phenomenon called Rayleigh scattering. Here's why:\\n\\n*   **Blue light is scattered:** Sunlight, which contains all the colors of the rainbow, is made up of particles of light. When sunlight hits the Earth's atmosphere, these particles scatter the light in all directions.\\n\\n*   **Rayleigh scattering:** The shorter wavelengths of light (blue and violet) are scattered more than longer wavelengths (red and orange). This is why we see blue skies.\\n\\n*   **The pattern of scattering:** The scattering pattern is uniform and continuous. As sunlight travels through the atmosphere, the scattered particles collide with the molecules of the air. These collisions cause the light to scatter in all directions, resulting in the characteristic blue color.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[105,2364,107,36425,563,506,7217,3730,236881,106,107,105,4368,107,818,7217,563,3730,2779,531,496,20284,2760,121707,19389,236761,5715,236789,236751,3217,236787,107,107,236829,138,5213,16520,2214,563,29892,53121,146430,236764,837,6097,784,506,7913,529,506,30591,236764,563,1603,872,529,10390,529,2214,236761,3026,26808,16737,506,10824,236789,236751,11661,236764,1239,10390,11887,506,2214,528,784,15232,236761,107,107,236829,138,5213,30958,53700,19389,53121,669,20532,57583,529,2214,568,9503,532,39261,236768,659,29892,919,1082,4890,57583,568,1192,532,11167,769,1174,563,3217,692,1460,3730,55748,236761,107,107,236829,138,5213,818,3759,529,19389,53121,669,19389,3759,563,9406,532,8906,236761,1773,26808,33036,1343,506,11661,236764,506,29892,10390,98230,607,506,13757,529,506,2634,236761,3143,39362,4400,506,2214,531,11887,528,784,15232,236764,9113,528,506,13629,3730,2258,236761],\"total_duration\":6038584658,\"load_duration\":3217034868,\"prompt_eval_count\":15,\"prompt_eval_duration\":68711747,\"eval_count\":154,\"eval_duration\":2471093868}"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\" \"$SERVICE_NAME\" \"$PROJECT_ID\" \"$REGION\"\n",
        "\n",
        "PROMPT=\"why is the sky blue?\"\n",
        "SERVICE_URL=$(gcloud run services describe \"$2\" --project \"$3\" --region \"$4\" --format 'value(status.url)' --quiet)\n",
        "AUTH_TOKEN=$(gcloud auth print-identity-token -q)\n",
        "\n",
        "curl -s -X POST \"${SERVICE_URL}/api/generate\" \\\n",
        "  -H \"Authorization: Bearer ${AUTH_TOKEN}\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d @<(cat <<EOF\n",
        "{\n",
        "  \"model\": \"$1\",\n",
        "  \"prompt\": \"$PROMPT\",\n",
        "  \"max_tokens\": 1000,\n",
        "  \"stream\": false\n",
        "}\n",
        "EOF\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63oQqBNmDDJC"
      },
      "source": [
        "### Ollama Python Library\n",
        "\n",
        "You can also use Ollama Python Library to make requests to the service you deployed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h8G3te5pDDJC"
      },
      "outputs": [],
      "source": [
        "# Install Ollama Python Library\n",
        "%pip install ollama -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X-2TbV6tDDJC",
        "outputId": "a617aa7f-f3d3-4ed4-ad04-4030f50778e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky is blue for a few key reasons:\n",
            "\n",
            "*   **Rayleigh Scattering:** Sunlight is composed of all the colors of the rainbow. When light enters the Earth's atmosphere, it collides with tiny air molecules. This collision causes the light to scatter in all directions.\n",
            "\n",
            "*   **Blue Light:** Blue light is scattered more than other colors. This is because blue light has a shorter wavelength and is scattered more strongly than other colors like red and orange.\n",
            "\n",
            "*   **Why the Sky is Blue:** The sun emits a lot of blue light. When the sun's light reaches the Earth's atmosphere, it collides with these air molecules. The scattering of blue light then returns to the Earth's surface, where it is scattered even more. This creates a rainbow-like effect, which is why we see the colors of the sky as blue.\n",
            "\n",
            "In summary, the blue color of the sky is a result of the scattering of blue light by the air molecules in the atmosphere, which is why we see the sky as blue."
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "from ollama import Client\n",
        "\n",
        "identity_token = (\n",
        "    subprocess.check_output(\"gcloud auth print-identity-token -q\", shell=True)\n",
        "    .decode()\n",
        "    .strip()\n",
        ")\n",
        "service_url = (\n",
        "    subprocess.check_output(\n",
        "        (\n",
        "            \"gcloud run services describe \"\n",
        "            f\"{SERVICE_NAME} --project={PROJECT_ID} \"\n",
        "            f\"--region={REGION} \"\n",
        "            \"--format='value(status.url)' -q\"\n",
        "        ),\n",
        "        shell=True,\n",
        "    )\n",
        "    .decode()\n",
        "    .strip()\n",
        ")\n",
        "client = Client(host=service_url, headers={\"Authorization\": f\"Bearer {identity_token}\"})\n",
        "stream = client.chat(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in stream:\n",
        "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_2CBkz0jwxb"
      },
      "source": [
        "### Using the python requests library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KL8AFOl0jwxb",
        "outputId": "0e13d527-7a00-441c-bc63-57d7e2f2376c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"model\":\"gemma3:270m\",\"created_at\":\"2025-11-08T09:07:39.671126328Z\",\"response\":\"The meaning of life is a question that has been pondered by humanity for millennia. There's no single, universally accepted answer, and the meaning of life is likely a deeply personal and subjective experience.\\n\",\"done\":true,\"done_reason\":\"stop\",\"context\":[105,2364,107,10979,236764,1144,563,506,6590,529,1972,236881,106,107,105,4368,107,818,6590,529,1972,563,496,2934,600,815,1010,198856,684,27069,573,164997,236761,2085,236789,236751,951,3161,236764,69127,10951,3890,236764,532,506,6590,529,1972,563,4547,496,19297,3577,532,44539,2707,236761,107],\"total_duration\":876366799,\"load_duration\":212035611,\"prompt_eval_count\":18,\"prompt_eval_duration\":30883696,\"eval_count\":42,\"eval_duration\":574274396}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {identity_token}\", \"Content-Type\": \"application/json\"}  # type: ignore\n",
        "\n",
        "data = {\n",
        "    \"model\": MODEL,\n",
        "    \"prompt\": \"Hi, what is the meaning of life?\",\n",
        "    \"max_tokens\": 100,\n",
        "    \"stream\": False,\n",
        "}\n",
        "service_url = (\n",
        "    subprocess.check_output(\n",
        "        (\n",
        "            \"gcloud run services describe \"\n",
        "            f\"{SERVICE_NAME} --project={PROJECT_ID} \"\n",
        "            f\"--region={REGION} \"\n",
        "            \"--format='value(status.url)' -q\"\n",
        "        ),\n",
        "        shell=True,\n",
        "    )\n",
        "    .decode()\n",
        "    .strip()\n",
        ")\n",
        "\n",
        "response = requests.post(f\"{service_url}/api/generate\", headers=headers, json=data)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsQDcSUCjwxb"
      },
      "source": [
        "### RAG Q&A Chain with Gemma 3 and Cloud Run\n",
        "\n",
        "We can leverage the LangChain integration to create a simple RAG application with Gemma, Cloud Run, Vertex AI Embedding for generating embeddings and SKLearnVectorStore which is a simple in-memory vector store based on scikit-learn's NearestNeighbors, using embeddings..\n",
        "\n",
        "Through RAG, we will ask Gemma 3 to answer questions about the Cloud Run documentation page\n",
        "\n",
        "### Setup embedding model and retriever\n",
        "\n",
        "We are ready to setup our embedding model and retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_EoZuKXjwxb"
      },
      "source": [
        "### Installed the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jf83nFv3jwxc"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet langchain-core langchain-community langchain_google_vertexai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eel7GBZ0jwxc"
      },
      "source": [
        "**Note**: If the above installation fails, do this;\n",
        "- Restart the session\n",
        "- Run the first cell again\n",
        "- Run the above cell again and it should work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKvd9fu_jwxc"
      },
      "source": [
        "### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_5jC0xl2jwxc",
        "outputId": "608c76a6-778c-474e-97c3-b6fd8fddcb83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "import subprocess\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "import google.auth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZZZFKWljwxc"
      },
      "source": [
        "### Setup Vertex AI Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p4ipayAtjwxc"
      },
      "outputs": [],
      "source": [
        "credentials, _ = google.auth.default(quota_project_id=PROJECT_ID)\n",
        "embeddings = VertexAIEmbeddings(\n",
        "    project=PROJECT_ID, model_name=\"text-embedding-005\", credentials=credentials\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtixXvOmjwxc"
      },
      "source": [
        "We will ground Gemini with content from Cloud Run Overview page. Load the content and store it in an in memory vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0t_4dmVajwxc",
        "outputId": "0445a454-3aeb-4464-faf9-23b7c7e05f65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1288, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1496, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 931, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 865, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1558, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 871, which is longer than the specified 800\n"
          ]
        }
      ],
      "source": [
        "loader = WebBaseLoader(\"https://cloud.google.com/run/docs/overview/what-is-cloud-run\")\n",
        "docs = loader.load()\n",
        "documents = CharacterTextSplitter(chunk_size=800, chunk_overlap=100).split_documents(\n",
        "    docs\n",
        ")\n",
        "\n",
        "vector = SKLearnVectorStore.from_documents(documents, embeddings)\n",
        "retriever = vector.as_retriever()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OHVW8g-jwxd"
      },
      "source": [
        "### RAG Chain Definition\n",
        "\n",
        "We will define now our RAG Chain.\n",
        "\n",
        "The RAG chain works as follows:\n",
        "\n",
        "- The users query is used by the retriever to fetch relevant documents.\n",
        "- The retrieved documents are formatted into a single string.\n",
        "- The formatted documents, along with the original user messages, are passed to the Gemma3 with instructions to generate an answer based on the provided context.\n",
        "- The LLM's response is parsed and returned as the final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kp7w1NcPjwxd"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate.from_template(\n",
        "    \"You are a helpful assistant.\\n\"\n",
        "    \"Answer using ONLY the context below.\\n\\n\"\n",
        "    \"CONTEXT:\\n{context}\\n\\n\"\n",
        "    \"QUESTION: {question}\\n\"\n",
        "    \"ANSWER:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "p3Ul0xuDjwxd"
      },
      "outputs": [],
      "source": [
        "question = \"What is Cloud Run and what problem does it solve?\"\n",
        "\n",
        "relevant_docs = retriever.invoke(question)\n",
        "joined_context = \"\\n\\n\".join(d.page_content for d in relevant_docs)\n",
        "\n",
        "final_prompt_str = prompt.format(context=joined_context, question=question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbVsxQaNjwxn"
      },
      "source": [
        "### Testing the RAG Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LDYngzkAjwxo",
        "outputId": "e28db277-d642-4841-d271-250d49014d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 2 column 1 (char 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.12/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[1;32m    513\u001b[0m             and not use_decimal and not allow_nan and not kw):\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1974360290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{service_url}/api/generate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mresp_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.12/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;31m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)"
          ]
        }
      ],
      "source": [
        "service_url = (\n",
        "    subprocess.check_output(\n",
        "        (\n",
        "            \"gcloud run services describe \"\n",
        "            f\"{SERVICE_NAME} --project={PROJECT_ID} \"\n",
        "            f\"--region={REGION} \"\n",
        "            \"--format='value(status.url)' -q\"\n",
        "        ),\n",
        "        shell=True,\n",
        "    )\n",
        "    .decode()\n",
        "    .strip()\n",
        ")\n",
        "payload = {\n",
        "    \"model\": MODEL,\n",
        "    \"prompt\": final_prompt_str,\n",
        "    \"stream\": False,\n",
        "}\n",
        "\n",
        "resp = requests.post(f\"{service_url}/api/generate\", json=payload)\n",
        "resp_json = resp.json()\n",
        "\n",
        "print(resp_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"SERVICE_NAME=$SERVICE_NAME\"\n",
        "!echo \"PROJECT_ID=$PROJECT_ID\"\n",
        "!echo \"REGION=$REGION\"\n"
      ],
      "metadata": {
        "id": "xd4EhsLNzZPn",
        "outputId": "24b411f2-db40-4def-8adb-9b85692412d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERVICE_NAME=ollama--gemma3-270m\n",
            "PROJECT_ID=sangalo-personal\n",
            "REGION=us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFL2V_ClDDJD"
      },
      "source": [
        "## Conclusion\n",
        "Congratulations! ðŸ’Ž Now you know how to deploy Gemma 3 to Cloud Run!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6f17f9aff65"
      },
      "source": [
        "## Cleaning up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1blF2ziDDJD"
      },
      "source": [
        "To delete the Cloud Run service you created, you can uncomment and run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VbhAz7-9DDJD",
        "outputId": "2fe2688b-34ad-4ba8-b09a-3ce96436ca33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted service [ollama--gemma3-270m].\n"
          ]
        }
      ],
      "source": [
        "!gcloud run services delete $SERVICE_NAME --project $PROJECT_ID --region $REGION --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "cloud_run_ollama_gemma3_inference.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}