{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangalo20/latitude/blob/main/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Serving Gemma 3 on Cloud Run\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/jmwai/gemma3-cloud-run-demo/blob/main/cloud_run_ollama_gemma3_inference.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/cloud_run_ollama_gemma3_inference.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83b98b0ba19c"
      },
      "source": [
        "<img src=\"https://ollama.com/public/ollama.png\" height=\"200px\" alignment=\"center\"/>\n",
        "<img src=\"https://cloud.google.com/static/architecture/images/ac-page-icons/card_google_cloud_partner.svg\" height=\"200px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Vlad Kolesnikov](https://github.com/vladkol) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccd500ae19b5"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "  [**Gemma 3**](https://ai.google.dev/gemma) is a new generation of open models developed by Google. It is a collection of lightweight, state-of-the-art open models built from the same research and technology that powers our Gemini 2.0 models. Gemma 3 comes in a range of sizes (270M, 1B, 4B, 12B and 27B), allowing you to choose the best model for your specific hardware and performance needs. Gemma 3 models are available through platforms like Google AI Studio, Vertex AI, Kaggle, and Hugging Face.\n",
        "\n",
        "> **[Cloud Run](https://cloud.google.com/run)**:\n",
        "It's a serverless platform by Google Cloud for running containerized applications. It automatically scales and manages infrastructure, supporting various programming languages. Cloud Run now offers GPU acceleration for AI/ML workloads. With 30 seconds to the first token, Cloud Run is a perfect platform for serving lightweight models like Gemma.\n",
        "\n",
        "> **Note:** GPU support in Cloud Run is in preview. To use the GPU feature, you must request `Total Nvidia L4 GPU allocation, per project per region` quota under Cloud Run in the [Quotas and system limits page](https://cloud.google.com/run/quotas#increase).\n",
        "\n",
        "\n",
        "> **[Ollama](ollama.com)**: is an open-source tool for easily running and deploying large language models locally. It offers simple management and usage of LLMs on personal computers or servers.\n",
        "\n",
        "This notebook showcases how to deploy [Google Gemma 3](https://developers.googleblog.com/en/introducing-gemma3) in Cloud Run, with the objective to build a simple API for chat or RAG applications.\n",
        "\n",
        "By the end of this notebook, you will learn how to:\n",
        "\n",
        "1. Deploy Google Gemma 3 as an OpenAI-compatible API on Cloud Run using Ollama.\n",
        "2. Build a custom container with Ollama to deploy any Large Language Model (LLM) of your choice.\n",
        "3. Make requests to an API hosted on Cloud Run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOiPjM5DEPhK"
      },
      "source": [
        "### Install Google Cloud SDK\n",
        "\n",
        "Make sure you Google Cloud SDK is installed (try running `gcloud version`) or [install it](https://cloud.google.com/sdk/docs/install) before executing this notebook.\n",
        "\n",
        "> If you are running in Colab or Vertex AI workbench, you have Google Cloud SDK installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfAVa08RDDJB"
      },
      "source": [
        "### Choose a model, a project, and a region to host the model\n",
        "\n",
        "Choose a Gemma 3 model to use, a Google Cloud project to host your Cloud Run service, and a region to host it in.\n",
        "For this demo we will chose the gemma3:270m model. If you cannot attach a GPU to your Cloud Run instance, chose the gemma3:270m and remove the GPU requirements in the cloud run command\n",
        "\n",
        "If you don't have a project yet:\n",
        "\n",
        "1. [Create a project](https://console.cloud.google.com/projectcreate) in the Google Cloud Console.\n",
        "2. Copy your `Project ID` from the project's [Settings page](https://console.cloud.google.com/iam-admin/settings).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TV0pbqJHDDJB"
      },
      "outputs": [],
      "source": [
        "# { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "\n",
        "PROJECT_ID = \"cloudrun-gemma-476809\"  # @param {type:\"string\", isTemplate: true}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\", isTemplate: true}\n",
        "MODEL = \"gemma3:270m\" # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "if PROJECT_ID == \"[your-project-id]\" or not PROJECT_ID:\n",
        "    print(\"Please specify your project id in PROJECT_ID variable.\")\n",
        "    raise KeyboardInterrupt\n",
        "\n",
        "MODEL_NAME_ESCAPED = MODEL.translate(str.maketrans(\".:/\", \"---\"))\n",
        "SERVICE_NAME = f\"ollama--{MODEL_NAME_ESCAPED}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_gThdop3ruF6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xc8Jm1P3Y7fs",
        "outputId": "a953957a-6fcb-4740-cb07-44b0d7956963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=M9r7azCwX5qEnsjYKS2JsNi1wQMhRK&prompt=consent&token_usage=remote&access_type=offline&code_challenge=uIhh11l_jkjKjwldx5BkEDk1rGs3sYahcc2c2oCqmys&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0Ab32j92JCH-DlfFSd-2F10Tc3kAL5vKi9HG6hx70QwXbOYz1qmj44M8T9t8FYV3s1qFdWA\n",
            "\n",
            "Application Default Credentials (ADC) were updated.\n",
            "\n",
            "You are now logged in as [mwenyinyo@gmail.com].\n",
            "Your current project is [cloudrun-gemma-476809].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth print-identity-token -q &> /dev/null || gcloud auth login --project=\"{PROJECT_ID}\" --update-adc --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l728UOEPDDJB"
      },
      "source": [
        "## Prepare container image\n",
        "\n",
        "First, let's create a Docker file for a container with the model embedded into it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "glBn9gPKDDJB",
        "outputId": "22cd69c0-7908-4697-97a0-8d8f06f17696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM ollama/ollama:latest\n",
        "\n",
        "ARG MODEL\n",
        "\n",
        "# Set the model name\n",
        "ENV MODEL=$MODEL\n",
        "\n",
        "# Set the host and port to listen on\n",
        "ENV OLLAMA_HOST 0.0.0.0:8080\n",
        "\n",
        "# Set the directory to store model weight files\n",
        "ENV OLLAMA_MODELS /models\n",
        "\n",
        "# Reduce the verbosity of the logs\n",
        "ENV OLLAMA_DEBUG false\n",
        "\n",
        "# Do not unload model weights from the GPU\n",
        "ENV OLLAMA_KEEP_ALIVE -1\n",
        "\n",
        "# Start the ollama server and download the model weights\n",
        "RUN ollama serve & sleep 5 && ollama pull $MODEL\n",
        "\n",
        "# At startup time we start the server and run a dummy request\n",
        "# to request the model to be loaded in the GPU memory\n",
        "ENTRYPOINT [\"/bin/sh\"]\n",
        "CMD [\"-c\", \"ollama serve  & (ollama run $MODEL 'Say one word' &) && wait\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4gS8ovMDDJB"
      },
      "source": [
        "Second, we create a Cloud Build file to use for building and pushing our container image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1dV1_cMDDDJB",
        "outputId": "547d58ba-4a5d-40ac-e7b3-c99db742674a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cloudbuild.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile cloudbuild.yaml\n",
        "\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  id: build\n",
        "  entrypoint: 'bash'\n",
        "  args:\n",
        "    - -c\n",
        "    - |\n",
        "        docker buildx build --tag=${_IMAGE} --build-arg MODEL=${_MODEL} .\n",
        "\n",
        "images: [\"${_IMAGE}\"]\n",
        "\n",
        "substitutions:\n",
        "  _IMAGE: '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_AR_REPO_NAME}/${_SERVICE_NAME}'\n",
        "\n",
        "options:\n",
        "  dynamicSubstitutions: true\n",
        "  machineType: \"E2_HIGHCPU_32\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbDiABJcDDJC"
      },
      "source": [
        "## Build Container Image and Deploy Cloud Run Service\n",
        "\n",
        "We are ready to build our container image and deploy Cloud Run service.\n",
        "\n",
        "The script below performs the following actions:\n",
        "\n",
        "* Enables necessary APIs.\n",
        "* Creates an Artifact Repository for the image.\n",
        "* Creates a Service Account for the service.\n",
        "* Submits a Cloud Build job to create and push the container image.\n",
        "* Deploys the Cloud Run service.\n",
        "\n",
        "> The script may take 10-45 minutes to finish.\n",
        "\n",
        "Note the following important flags in Cloud Build deployment command:\n",
        "\n",
        "* `--concurrency 4` is set to match the value of the environment variable `OLLAMA_NUM_PARALLEL`.\n",
        "* `--gpu 1` with `--gpu-type nvidia-l4` assigns 1 NVIDIA L4 GPU to every Cloud Run instance in the service.\n",
        "`--no-allow-authenticated` restricts unauthenticated access to the service.\n",
        "By keeping the service private, you can rely on Cloud Run's built-in [Identity and Access Management (IAM)](https://cloud.google.com/iam) authentication for service-to-service communication.\n",
        "* `--no-cpu-throttling` is required for enabling GPU.\n",
        "* `--service-account` the service identity of the service.\n",
        "* `--max-instances` sets maximum number of instances of the service.\n",
        "It has to be equal to or lower than your project's NVIDIA L4 GPU (`Total Nvidia L4 GPU allocation, per project per region`) quota.\n",
        "\n",
        "For optimal GPU utilization, increase `--concurrency`, keeping it within twice the value of `OLLAMA_NUM_PARALLEL`.\n",
        "While this leads to request queuing in Ollama, it can help improve utilization:\n",
        "Ollama instances can immediately process requests from their queue, and the queues help absorb traffic spikes.\n",
        "\n",
        "#### If your cloud credits don't allow you to attache a GPU, change to Gemma 270m variant and deploy without GPU requirement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TXg7IYU1DDJC",
        "outputId": "7328809c-565f-465a-a7bb-6b8879754fbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting deploy.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile deploy.sh\n",
        "\n",
        "PROJECT_ID=$1\n",
        "REGION=$2\n",
        "MODEL_ID=\"${3}\"\n",
        "SERVICE_NAME=\"${4}\"\n",
        "AR_REPO_NAME=\"ollama-repo\"\n",
        "SERVICE_ACCOUNT=\"ollama-cloud-run-sa\"\n",
        "SERVICE_ACCOUNT_ADDRESS=\"${SERVICE_ACCOUNT}@$PROJECT_ID.iam.gserviceaccount.com\"\n",
        "MAX_INSTANCES=1 # Adjust this value to match your Cloud Run L4 GPU quota (\"Total Nvidia L4 GPU allocation, per project per region\", NvidiaL4GpuAllocPerProjectRegion, run.googleapis.com/nvidia_l4_gpu_allocation)\n",
        "\n",
        "echo \"Enabling APIs in project ${PROJECT_ID}.\"\n",
        "gcloud services enable run.googleapis.com \\\n",
        "    cloudbuild.googleapis.com \\\n",
        "    artifactregistry.googleapis.com \\\n",
        "    --project ${PROJECT_ID} \\\n",
        "    --quiet\n",
        "\n",
        "set -e\n",
        "\n",
        "# Creating the service account if doesn't exist.\n",
        "sa_list=$(gcloud iam service-accounts list --quiet --format 'value(email)' --project $PROJECT_ID --filter=email:$SERVICE_ACCOUNT@$PROJECT_ID.iam.gserviceaccount.com 2>/dev/null)\n",
        "if [ -z \"${sa_list}\" ]; then\n",
        "    echo \"Creating Service Account ${SERVICE_ACCOUNT}.\"\n",
        "    gcloud iam service-accounts create $SERVICE_ACCOUNT \\\n",
        "        --project ${PROJECT_ID} \\\n",
        "        --display-name=\"${SERVICE_ACCOUNT} - Cloud Run Service Account\"\n",
        "fi\n",
        "\n",
        "# Creating the Artifacts Repository if doesn't exist\n",
        "repo_list=$(gcloud artifacts repositories list --format 'value(name)' --filter=name=\"projects/${PROJECT_ID}/locations/${REGION}/repositories/${AR_REPO_NAME}\" --project ${PROJECT_ID} --quiet --location ${REGION} 2>/dev/null)\n",
        "if [ -z \"${repo_list}\" ]; then\n",
        "    echo \"Creating Artifact Registry ${AR_REPO_NAME}.\"\n",
        "    gcloud artifacts repositories create $AR_REPO_NAME \\\n",
        "    --repository-format docker \\\n",
        "    --location ${REGION} \\\n",
        "    --project=${PROJECT_ID}\n",
        "fi\n",
        "\n",
        "echo \"Building container image.\"\n",
        "gcloud builds submit --config=cloudbuild.yaml --project=${PROJECT_ID} . \\\n",
        "    --suppress-logs \\\n",
        "    --substitutions \\\n",
        "  _AR_REPO_NAME=$AR_REPO_NAME,_REGION=$REGION,_SERVICE_NAME=$SERVICE_NAME,_MODEL=$MODEL_ID\n",
        "rm -f cloudbuild.yaml\n",
        "rm -f Dockerfile\n",
        "\n",
        "echo \"Deploying Service ${SERVICE_NAME}.\"\n",
        "gcloud beta run deploy $SERVICE_NAME \\\n",
        "    --project=${PROJECT_ID} \\\n",
        "    --image=${REGION}-docker.pkg.dev/$PROJECT_ID/$AR_REPO_NAME/$SERVICE_NAME \\\n",
        "    --service-account $SERVICE_ACCOUNT_ADDRESS \\\n",
        "    --cpu=8 \\\n",
        "    --memory=32Gi \\\n",
        "    --set-env-vars OLLAMA_NUM_PARALLEL=4 \\\n",
        "    --region ${REGION} \\\n",
        "    --no-allow-unauthenticated \\\n",
        "    --max-instances ${MAX_INSTANCES} \\\n",
        "    --no-cpu-throttling \\\n",
        "    --timeout 1h\n",
        "\n",
        "SERVICE_URL=$(gcloud run services describe ${SERVICE_NAME} --project=${PROJECT_ID} --region $REGION --format 'value(status.url)' --quiet)\n",
        "echo \"âœ… Success!\"\n",
        "echo \"ðŸš€ Service URL: ${SERVICE_URL}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e6L2dVGyOAxB",
        "outputId": "ba153780-e09b-4f39-a1e5-20c56735d644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling APIs in project cloudrun-gemma-476809.\n",
            "Operation \"operations/acat.p2-781254510635-b415c057-aa95-443b-a84b-0b25352a274a\" finished successfully.\n",
            "Building container image.\n",
            "Creating temporary archive of 49 file(s) totalling 54.4 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://cloudrun-gemma-476809_cloudbuild/source/1761904723.495307-bb4d4a5c020f4cf88fc7a09758ba59ce.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/cloudrun-gemma-476809/locations/global/builds/9e459553-69a1-4155-9a55-f52d9f3f8859].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/9e459553-69a1-4155-9a55-f52d9f3f8859?project=781254510635 ].\n",
            "Waiting for build to complete. Polling interval: 1 second(s).\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                                                                      STATUS\n",
            "9e459553-69a1-4155-9a55-f52d9f3f8859  2025-10-31T09:58:54+00:00  3M42S     gs://cloudrun-gemma-476809_cloudbuild/source/1761904723.495307-bb4d4a5c020f4cf88fc7a09758ba59ce.tgz  us-central1-docker.pkg.dev/cloudrun-gemma-476809/ollama-repo/ollama--gemma3-270m (+1 more)  SUCCESS\n",
            "Deploying Service ollama--gemma3-270m.\n",
            "Deploying container to Cloud Run service [\u001b[1mollama--gemma3-270m\u001b[m] in project [\u001b[1mcloudrun-gemma-476809\u001b[m] region [\u001b[1mus-central1\u001b[m]\n",
            "Service [\u001b[1mollama--gemma3-270m\u001b[m] revision [\u001b[1mollama--gemma3-270m-00001-hbc\u001b[m] has been deployed and is serving \u001b[1m100\u001b[m percent of traffic.\n",
            "Service URL: \u001b[1mhttps://ollama--gemma3-270m-781254510635.us-central1.run.app\u001b[m\n",
            "âœ… Success!\n",
            "ðŸš€ Service URL: https://ollama--gemma3-270m-totgkomwfq-uc.a.run.app\n"
          ]
        }
      ],
      "source": [
        "!/bin/bash ./deploy.sh \"{PROJECT_ID}\" \"{REGION}\" \"{MODEL}\" \"{SERVICE_NAME}\" && rm -f ./deploy.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgaJ62rmDDJC"
      },
      "source": [
        "\n",
        "## Test the deployed service\n",
        "\n",
        "Now, let's test the service you deployed.\n",
        "\n",
        "First, simply by using `cURL`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iX7LmWwGDDJC",
        "outputId": "a0991c6e-f4d1-493b-f742-c60645aa1c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"model\":\"gemma3:270m\",\"created_at\":\"2025-10-31T10:05:28.917449347Z\",\"response\":\"The sky is blue for a few key reasons:\\n\\n*   **Rayleigh Scattering:** Sunlight is composed of all colors of light. When these colors are scattered by air molecules, we see the scattered light as blue. This is why the sky appears blue.\\n*   **Blue Light:** The sun emits a continuous range of light colors. The blue light is scattered more than other colors, so the sky is blue.\\n*   **Absorption and Reflection:** The air molecules absorb some of the blue light, but they also reflect some of it back to the Earth. This is why the sky appears blue.\\n*   **Atmospheric Conditions:** The atmosphere also absorbs and scatters some of the blue light, making the sky appear blue.\\n\\nIn summary, the sky is blue because of the scattering of light by the air molecules, which is what gives it its blue color.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[105,2364,107,36425,563,506,7217,3730,236881,106,107,105,4368,107,818,7217,563,3730,573,496,2321,2307,7483,236787,107,107,236829,138,5213,30958,53700,178868,53121,146430,563,16635,529,784,7913,529,2214,236761,3026,1239,7913,659,29892,684,2634,13757,236764,692,1460,506,29892,2214,618,3730,236761,1174,563,3217,506,7217,7412,3730,236761,107,236829,138,5213,16520,10847,53121,669,3768,80375,496,8906,2644,529,2214,7913,236761,669,3730,2214,563,29892,919,1082,1032,7913,236764,834,506,7217,563,3730,236761,107,236829,138,5213,202095,532,92103,53121,669,2634,13757,22792,1070,529,506,3730,2214,236764,840,901,992,7495,1070,529,625,1063,531,506,10824,236761,1174,563,3217,506,7217,7412,3730,236761,107,236829,138,5213,165451,39091,32288,53121,669,11661,992,81757,532,141891,1826,1070,529,506,3730,2214,236764,3043,506,7217,3196,3730,236761,107,107,902,12323,236764,506,7217,563,3730,1547,529,506,19389,529,2214,684,506,2634,13757,236764,837,563,1144,5021,625,1061,3730,2258,236761],\"total_duration\":6110812119,\"load_duration\":236844101,\"prompt_eval_count\":15,\"prompt_eval_duration\":91084153,\"eval_count\":179,\"eval_duration\":5522725329}"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\" \"$SERVICE_NAME\" \"$PROJECT_ID\" \"$REGION\"\n",
        "\n",
        "PROMPT=\"why is the sky blue?\"\n",
        "SERVICE_URL=$(gcloud run services describe \"$2\" --project \"$3\" --region \"$4\" --format 'value(status.url)' --quiet)\n",
        "AUTH_TOKEN=$(gcloud auth print-identity-token -q)\n",
        "\n",
        "curl -s -X POST \"${SERVICE_URL}/api/generate\" \\\n",
        "  -H \"Authorization: Bearer ${AUTH_TOKEN}\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d @<(cat <<EOF\n",
        "{\n",
        "  \"model\": \"$1\",\n",
        "  \"prompt\": \"$PROMPT\",\n",
        "  \"max_tokens\": 1000,\n",
        "  \"stream\": false\n",
        "}\n",
        "EOF\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63oQqBNmDDJC"
      },
      "source": [
        "### Ollama Python Library\n",
        "\n",
        "You can also use Ollama Python Library to make requests to the service you deployed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h8G3te5pDDJC"
      },
      "outputs": [],
      "source": [
        "# Install Ollama Python Library\n",
        "%pip install ollama -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X-2TbV6tDDJC",
        "outputId": "30069691-7773-4b77-8058-897fb2e26acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky is blue due to a phenomenon called Rayleigh scattering. \n",
            "\n",
            "Here's the breakdown:\n",
            "\n",
            "*   **Sunlight:** Sunlight is actually made up of all the colors of the rainbow.\n",
            "*   **Blue Light:** Blue light has a shorter wavelength than other colors.\n",
            "*   **Rayleigh Scattering:** When sunlight enters the atmosphere, it collides with tiny particles like dust and air molecules. These particles scatter the light in all directions.\n",
            "*   **Blue Light Scattered:** The scattered blue light is then slowed down by the molecules in the atmosphere, which are much smaller than the wavelengths of blue light.\n",
            "*   **Why Blue?** Because blue light has a much shorter wavelength than other colors. This means that most of the blue light is scattered away from the Earth's surface by the air molecules.\n",
            "\n",
            "So, the sky appears blue because the blue light is scattered away by the air molecules."
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "from ollama import Client\n",
        "\n",
        "identity_token = (\n",
        "    subprocess.check_output(\"gcloud auth print-identity-token -q\", shell=True)\n",
        "    .decode()\n",
        "    .strip()\n",
        ")\n",
        "service_url = (\n",
        "    subprocess.check_output(\n",
        "        (\n",
        "            \"gcloud run services describe \"\n",
        "            f\"{SERVICE_NAME} --project={PROJECT_ID} \"\n",
        "            f\"--region={REGION} \"\n",
        "            \"--format='value(status.url)' -q\"\n",
        "        ),\n",
        "        shell=True,\n",
        "    )\n",
        "    .decode()\n",
        "    .strip()\n",
        ")\n",
        "client = Client(host=service_url, headers={\"Authorization\": f\"Bearer {identity_token}\"})\n",
        "stream = client.chat(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in stream:\n",
        "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DrM6CaCrSoc"
      },
      "source": [
        "### Using the python requests library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Zf8ZcnOSrSod",
        "outputId": "ce80716d-3582-4b4a-fc87-a7b8b210702e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"model\":\"gemma3:270m\",\"created_at\":\"2025-10-31T10:06:56.689470112Z\",\"response\":\"Ah, the million-dollar question! The meaning of life is a deeply personal and philosophical question that has plagued humanity for centuries. There's no single, universally accepted answer. \\n\\nHere are some common perspectives and approaches to understanding the meaning of life:\\n\\n*   **Purpose and Meaning:** This is often the most common answer. It suggests that life has a purpose, whether it's fulfilling a specific goal, contributing to something larger than oneself, or finding meaning in a particular activity.\\n*   **Relationships and Connection:** Meaning often arises from strong relationships with others, strong communities, and a sense of belonging.\\n*   **Growth and Learning:** Life is a journey of continuous learning and growth. We need to strive to become better, develop new skills, and expand our horizons.\\n*   **Experiences and Adventure:** Embracing new experiences, pursuing passions, and traveling the world can provide a sense of fulfillment and purpose.\\n*   **Contribution and Service:** Helping others, making a positive impact on the world, and contributing to something meaningful can be a source of deep satisfaction.\\n*   **Finding Your Values:** Identifying your core values and aligning your actions with them can help you create a meaningful life.\\n*   **Acceptance and Letting Go:** Accepting the present moment and letting go of the past or future can lead to a more peaceful and fulfilling existence.\\n\\nUltimately, the meaning of life is what you make it. It's a journey of exploration, discovery, and personal growth. What resonates with you might not be a definitive answer, and that's perfectly okay!\",\"done\":true,\"done_reason\":\"stop\",\"context\":[105,2364,107,10979,236764,1144,563,506,6590,529,1972,236881,106,107,105,4368,107,24019,236764,506,3625,236772,63811,2934,236888,669,6590,529,1972,563,496,19297,3577,532,52491,2934,600,815,95563,27069,573,24744,236761,2085,236789,236751,951,3161,236764,69127,10951,3890,236761,236743,107,107,8291,659,1070,3364,32557,532,12668,531,6611,506,6590,529,1972,236787,107,107,236829,138,5213,72655,532,54076,53121,1174,563,3187,506,1346,3364,3890,236761,1030,12205,600,1972,815,496,5708,236764,3363,625,236789,236751,51577,496,3530,5671,236764,20894,531,2613,6268,1082,64584,236764,653,8159,6590,528,496,2931,4588,236761,107,236829,138,5213,134564,532,27618,53121,54076,3187,30952,699,3188,9994,607,3496,236764,3188,8904,236764,532,496,5113,529,24898,236761,107,236829,138,5213,83380,532,19180,53121,8586,563,496,9338,529,8906,4735,532,3877,236761,1191,1202,531,36438,531,3291,2480,236764,1668,861,6130,236764,532,6778,1023,85338,236761,107,236829,138,5213,222178,532,47880,53121,221277,4761,861,9871,236764,35778,62021,236764,532,18154,506,1902,740,2847,496,5113,529,56470,532,5708,236761,107,236829,138,5213,183339,532,6007,53121,100781,3496,236764,3043,496,4414,4100,580,506,1902,236764,532,20894,531,2613,21475,740,577,496,3738,529,5268,17681,236761,107,236829,138,5213,34097,5180,35254,53121,120570,822,7157,2979,532,120022,822,7419,607,1091,740,1601,611,2619,496,21475,1972,236761,107,236829,138,5213,24040,831,532,172081,3764,53121,233793,506,1861,3479,532,23725,817,529,506,3068,653,3402,740,2080,531,496,919,26169,532,51577,10664,236761,107,107,91607,236764,506,6590,529,1972,563,1144,611,1386,625,236761,1030,236789,236751,496,9338,529,22824,236764,17324,236764,532,3577,3877,236761,2900,119636,607,611,2473,711,577,496,55290,3890,236764,532,600,236789,236751,13275,10649,236888],\"total_duration\":10837089973,\"load_duration\":194130614,\"prompt_eval_count\":18,\"prompt_eval_duration\":114877616,\"eval_count\":326,\"eval_duration\":10009182539}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {identity_token}\", \"Content-Type\": \"application/json\"}  # type: ignore\n",
        "\n",
        "data = {\n",
        "    \"model\": MODEL,\n",
        "    \"prompt\": \"Hi, what is the meaning of life?\",\n",
        "    \"max_tokens\": 100,\n",
        "    \"stream\": False,\n",
        "}\n",
        "service_url = (\n",
        "    subprocess.check_output(\n",
        "        (\n",
        "            \"gcloud run services describe \"\n",
        "            f\"{SERVICE_NAME} --project={PROJECT_ID} \"\n",
        "            f\"--region={REGION} \"\n",
        "            \"--format='value(status.url)' -q\"\n",
        "        ),\n",
        "        shell=True,\n",
        "    )\n",
        "    .decode()\n",
        "    .strip()\n",
        ")\n",
        "\n",
        "response = requests.post(f\"{service_url}/api/generate\", headers=headers, json=data)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aTTl4ecrSoe"
      },
      "source": [
        "### RAG Q&A Chain with Gemma 3 and Cloud Run\n",
        "\n",
        "We can leverage the LangChain integration to create a simple RAG application with Gemma, Cloud Run, Vertex AI Embedding for generating embeddings and SKLearnVectorStore which is a simple in-memory vector store based on scikit-learn's NearestNeighbors, using embeddings..\n",
        "\n",
        "Through RAG, we will ask Gemma 3 to answer questions about the Cloud Run documentation page\n",
        "\n",
        "### Setup embedding model and retriever\n",
        "\n",
        "We are ready to setup our embedding model and retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvs6KNKirSof"
      },
      "source": [
        "### Installed the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zxrTJVZ0rSog"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet langchain-core langchain-community langchain_google_vertexai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "giQ082KDv3cJ",
        "outputId": "2fe29d07-c6a6-4c49-e4a1-a21d4d50db0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6ZskH6hrSoh"
      },
      "source": [
        "**Note**: If the above installation fails, do this;\n",
        "- Restart the session\n",
        "- Run the first cell again\n",
        "- Run the above cell again and it should work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKqZ2nmorSoh"
      },
      "source": [
        "### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1EREBtxvrSoi",
        "outputId": "e69177d7-0024-4dee-974c-7dd45a3f8f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "import subprocess\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "import google.auth\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "\n",
        "PROJECT_ID = \"cloudrun-gemma-476809\"  # @param {type:\"string\", isTemplate: true}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\", isTemplate: true}\n",
        "MODEL = \"gemma3:270m\" # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "if PROJECT_ID == \"[your-project-id]\" or not PROJECT_ID:\n",
        "    print(\"Please specify your project id in PROJECT_ID variable.\")\n",
        "    raise KeyboardInterrupt\n",
        "\n",
        "MODEL_NAME_ESCAPED = MODEL.translate(str.maketrans(\".:/\", \"---\"))\n",
        "SERVICE_NAME = f\"ollama--{MODEL_NAME_ESCAPED}\""
      ],
      "metadata": {
        "id": "650BthjQwfk3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kDO8XH0rSoj"
      },
      "source": [
        "### Setup Vertex AI Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l5VojPg_rSoj"
      },
      "outputs": [],
      "source": [
        "credentials, _ = google.auth.default(quota_project_id=PROJECT_ID)\n",
        "embeddings = VertexAIEmbeddings(\n",
        "    project=PROJECT_ID, model_name=\"text-embedding-005\", credentials=credentials\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVBlwl1srSok"
      },
      "source": [
        "We will ground Gemini with content from Cloud Run Overview page. Load the content and store it in an in memory vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ygmLmYAYrSox",
        "outputId": "de3613b3-9835-42d6-969e-4906f01ac26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1288, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1496, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 931, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 865, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1558, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 871, which is longer than the specified 800\n"
          ]
        }
      ],
      "source": [
        "loader = WebBaseLoader(\"https://cloud.google.com/run/docs/overview/what-is-cloud-run\")\n",
        "docs = loader.load()\n",
        "documents = CharacterTextSplitter(chunk_size=800, chunk_overlap=100).split_documents(\n",
        "    docs\n",
        ")\n",
        "\n",
        "vector = SKLearnVectorStore.from_documents(documents, embeddings)\n",
        "retriever = vector.as_retriever()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l4GbKhHrSox"
      },
      "source": [
        "### RAG Chain Definition\n",
        "\n",
        "We will define now our RAG Chain.\n",
        "\n",
        "The RAG chain works as follows:\n",
        "\n",
        "- The users query is used by the retriever to fetch relevant documents.\n",
        "- The retrieved documents are formatted into a single string.\n",
        "- The formatted documents, along with the original user messages, are passed to the Gemma3 with instructions to generate an answer based on the provided context.\n",
        "- The LLM's response is parsed and returned as the final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8Bsp8Su2rSoy"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate.from_template(\n",
        "    \"You are a helpful assistant.\\n\"\n",
        "    \"Answer using ONLY the context below.\\n\\n\"\n",
        "    \"CONTEXT:\\n{context}\\n\\n\"\n",
        "    \"QUESTION: {question}\\n\"\n",
        "    \"ANSWER:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WWox1GKcrSo0"
      },
      "outputs": [],
      "source": [
        "question = \"What is Cloud Run and what problem does it solve?\"\n",
        "\n",
        "relevant_docs = retriever.invoke(question)\n",
        "joined_context = \"\\n\\n\".join(d.page_content for d in relevant_docs)\n",
        "\n",
        "final_prompt_str = prompt.format(context=joined_context, question=question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNL6PcAdrSpF"
      },
      "source": [
        "### Testing the RAG Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haYbxPUArSpG"
      },
      "outputs": [],
      "source": [
        "service_url = (\n",
        "    subprocess.check_output(\n",
        "        (\n",
        "            \"gcloud run services describe \"\n",
        "            f\"{SERVICE_NAME} --project={PROJECT_ID} \"\n",
        "            f\"--region={REGION} \"\n",
        "            \"--format='value(status.url)' -q\"\n",
        "        ),\n",
        "        shell=True,\n",
        "    )\n",
        "    .decode()\n",
        "    .strip()\n",
        ")\n",
        "payload = {\n",
        "    \"model\": MODEL,\n",
        "    \"prompt\": final_prompt_str,\n",
        "    \"stream\": False,\n",
        "}\n",
        "\n",
        "resp = requests.post(f\"{service_url}/api/generate\", json=payload)\n",
        "resp_json = resp.json()\n",
        "\n",
        "print(resp_json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFL2V_ClDDJD"
      },
      "source": [
        "## Conclusion\n",
        "Congratulations! ðŸ’Ž Now you know how to deploy Gemma 3 to Cloud Run!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6f17f9aff65"
      },
      "source": [
        "## Cleaning up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1blF2ziDDJD"
      },
      "source": [
        "To delete the Cloud Run service you created, you can uncomment and run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbhAz7-9DDJD"
      },
      "outputs": [],
      "source": [
        "# !gcloud run services delete $SERVICE_NAME --project $PROJECT_ID --region $LOCATION --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "cloud_run_ollama_gemma3_inference.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}